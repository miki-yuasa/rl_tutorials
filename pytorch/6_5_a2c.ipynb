{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# パッケージのimport\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 定数の設定\r\n",
    "ENV = 'CartPole-v0'  # 使用する課題名\r\n",
    "GAMMA = 0.99  # 時間割引率\r\n",
    "MAX_STEPS = 200  # 1試行のstep数\r\n",
    "NUM_EPISODES = 1000  # 最大試行回数\r\n",
    "\r\n",
    "NUM_PROCESSES = 32  # 同時に実行する環境\r\n",
    "NUM_ADVANCED_STEP = 5  # 何ステップ進めて報酬和を計算するのか設定"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "# A2Cの損失関数の計算のための定数設定\r\n",
    "value_loss_coef = 0.5\r\n",
    "entropy_coef = 0.01\r\n",
    "max_grad_norm = 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# メモリクラスの定義\r\n",
    "\r\n",
    "\r\n",
    "class RolloutStorage(object):\r\n",
    "    '''Advantage学習するためのメモリクラスです'''\r\n",
    "\r\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\r\n",
    "\r\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, 4)\r\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\r\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\r\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\r\n",
    "\r\n",
    "        # 割引報酬和を格納\r\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\r\n",
    "        self.index = 0  # insertするインデックス\r\n",
    "\r\n",
    "    def insert(self, current_obs, action, reward, mask):\r\n",
    "        '''次のindexにtransitionを格納する'''\r\n",
    "        self.observations[self.index + 1].copy_(current_obs)\r\n",
    "        self.masks[self.index + 1].copy_(mask)\r\n",
    "        self.rewards[self.index].copy_(reward)\r\n",
    "        self.actions[self.index].copy_(action)\r\n",
    "\r\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # インデックスの更新\r\n",
    "\r\n",
    "    def after_update(self):\r\n",
    "        '''Advantageするstep数が完了したら、最新のものをindex0に格納'''\r\n",
    "        self.observations[0].copy_(self.observations[-1])\r\n",
    "        self.masks[0].copy_(self.masks[-1])\r\n",
    "\r\n",
    "    def compute_returns(self, next_value):\r\n",
    "        '''Advantageするステップ中の各ステップの割引報酬和を計算する'''\r\n",
    "\r\n",
    "        # 注意：5step目から逆向きに計算しています\r\n",
    "        # 注意：5step目はAdvantage1となる。4ステップ目はAdvantage2となる。・・・\r\n",
    "        self.returns[-1] = next_value\r\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\r\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * \\\r\n",
    "                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# A2Cのディープ・ニューラルネットワークの構築\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "\r\n",
    "class Net(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, n_in, n_mid, n_out):\r\n",
    "        super(Net, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\r\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\r\n",
    "        self.actor = nn.Linear(n_mid, n_out)  # 行動を決めるので出力は行動の種類数\r\n",
    "        self.critic = nn.Linear(n_mid, 1)  # 状態価値なので出力は1つ\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        '''ネットワークのフォワード計算を定義します'''\r\n",
    "        h1 = F.relu(self.fc1(x))\r\n",
    "        h2 = F.relu(self.fc2(h1))\r\n",
    "        critic_output = self.critic(h2)  # 状態価値の計算\r\n",
    "        actor_output = self.actor(h2)  # 行動の計算\r\n",
    "\r\n",
    "        return critic_output, actor_output\r\n",
    "\r\n",
    "    def act(self, x):\r\n",
    "        '''状態xから行動を確率的に求めます'''\r\n",
    "        value, actor_output = self(x)\r\n",
    "        # dim=1で行動の種類方向にsoftmaxを計算\r\n",
    "        action_probs = F.softmax(actor_output, dim=1)\r\n",
    "        action = action_probs.multinomial(num_samples=1)  # dim=1で行動の種類方向に確率計算\r\n",
    "        return action\r\n",
    "\r\n",
    "    def get_value(self, x):\r\n",
    "        '''状態xから状態価値を求めます'''\r\n",
    "        value, actor_output = self(x)\r\n",
    "\r\n",
    "        return value\r\n",
    "\r\n",
    "    def evaluate_actions(self, x, actions):\r\n",
    "        '''状態xから状態価値、実際の行動actionsのlog確率とエントロピーを求めます'''\r\n",
    "        value, actor_output = self(x)\r\n",
    "\r\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)  # dim=1で行動の種類方向に計算\r\n",
    "        action_log_probs = log_probs.gather(1, actions)  # 実際の行動のlog_probsを求める\r\n",
    "\r\n",
    "        probs = F.softmax(actor_output, dim=1)  # dim=1で行動の種類方向に計算\r\n",
    "        entropy = -(log_probs * probs).sum(-1).mean()\r\n",
    "\r\n",
    "        return value, action_log_probs, entropy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# エージェントが持つ頭脳となるクラスを定義、全エージェントで共有する\r\n",
    "import torch\r\n",
    "from torch import optim\r\n",
    "\r\n",
    "\r\n",
    "class Brain(object):\r\n",
    "    def __init__(self, actor_critic):\r\n",
    "        self.actor_critic = actor_critic  # actor_criticはクラスNetのディープ・ニューラルネットワーク\r\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)\r\n",
    "\r\n",
    "    def update(self, rollouts):\r\n",
    "        '''Advantageで計算した5つのstepの全てを使って更新します'''\r\n",
    "        obs_shape = rollouts.observations.size()[2:]  # torch.Size([4, 84, 84])\r\n",
    "        num_steps = NUM_ADVANCED_STEP\r\n",
    "        num_processes = NUM_PROCESSES\r\n",
    "\r\n",
    "        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\r\n",
    "            rollouts.observations[:-1].view(-1, 4),\r\n",
    "            rollouts.actions.view(-1, 1))\r\n",
    "\r\n",
    "        # 注意：各変数のサイズ\r\n",
    "        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\r\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\r\n",
    "        # values torch.Size([80, 1])\r\n",
    "        # action_log_probs torch.Size([80, 1])\r\n",
    "        # entropy torch.Size([])\r\n",
    "\r\n",
    "        values = values.view(num_steps, num_processes,\r\n",
    "                             1)  # torch.Size([5, 16, 1])\r\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\r\n",
    "\r\n",
    "        # advantage（行動価値-状態価値）の計算\r\n",
    "        advantages = rollouts.returns[:-1] - values  # torch.Size([5, 16, 1])\r\n",
    "\r\n",
    "        # Criticのlossを計算\r\n",
    "        value_loss = advantages.pow(2).mean()\r\n",
    "\r\n",
    "        # Actorのgainを計算、あとでマイナスをかけてlossにする\r\n",
    "        action_gain = (action_log_probs*advantages.detach()).mean()\r\n",
    "        # detachしてadvantagesを定数として扱う\r\n",
    "\r\n",
    "        # 誤差関数の総和\r\n",
    "        total_loss = (value_loss * value_loss_coef -\r\n",
    "                      action_gain - entropy * entropy_coef)\r\n",
    "\r\n",
    "        # 結合パラメータを更新\r\n",
    "        self.actor_critic.train()  # 訓練モードに\r\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\r\n",
    "        total_loss.backward()  # バックプロパゲーションを計算\r\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\r\n",
    "        #  一気に結合パラメータが変化しすぎないように、勾配の大きさは最大0.5までにする\r\n",
    "\r\n",
    "        self.optimizer.step()  # 結合パラメータを更新"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 実行する環境のクラスです\r\n",
    "import copy\r\n",
    "\r\n",
    "\r\n",
    "class Environment:\r\n",
    "    def run(self):\r\n",
    "        '''メインの実行'''\r\n",
    "\r\n",
    "        # 同時実行する環境数分、envを生成\r\n",
    "        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\r\n",
    "\r\n",
    "        # 全エージェントが共有して持つ頭脳Brainを生成\r\n",
    "        n_in = envs[0].observation_space.shape[0]  # 状態は4\r\n",
    "        n_out = envs[0].action_space.n  # 行動は2\r\n",
    "        n_mid = 32\r\n",
    "        actor_critic = Net(n_in, n_mid, n_out)  # ディープ・ニューラルネットワークの生成\r\n",
    "        global_brain = Brain(actor_critic)\r\n",
    "\r\n",
    "        # 格納用変数の生成\r\n",
    "        obs_shape = n_in\r\n",
    "        current_obs = torch.zeros(\r\n",
    "            NUM_PROCESSES, obs_shape)  # torch.Size([16, 4])\r\n",
    "        rollouts = RolloutStorage(\r\n",
    "            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)  # rolloutsのオブジェクト\r\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])  # 現在の試行の報酬を保持\r\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])  # 最後の試行の報酬を保持\r\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])  # Numpy配列\r\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])  # Numpy配列\r\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])  # Numpy配列\r\n",
    "        each_step = np.zeros(NUM_PROCESSES)  # 各環境のstep数を記録\r\n",
    "        episode = 0  # 環境0の試行数\r\n",
    "\r\n",
    "        # 初期状態の開始\r\n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\r\n",
    "        obs = np.array(obs)\r\n",
    "        obs = torch.from_numpy(obs).float()  # torch.Size([16, 4])\r\n",
    "        current_obs = obs  # 最新のobsを格納\r\n",
    "\r\n",
    "        # advanced学習用のオブジェクトrolloutsの状態の1つ目に、現在の状態を保存\r\n",
    "        rollouts.observations[0].copy_(current_obs)\r\n",
    "\r\n",
    "        # 実行ループ\r\n",
    "        for j in range(NUM_EPISODES*NUM_PROCESSES):  # 全体のforループ\r\n",
    "            # advanced学習するstep数ごとに計算\r\n",
    "            for step in range(NUM_ADVANCED_STEP):\r\n",
    "\r\n",
    "                # 行動を求める\r\n",
    "                with torch.no_grad():\r\n",
    "                    action = actor_critic.act(rollouts.observations[step])\r\n",
    "\r\n",
    "                # (16,1)→(16,)→tensorをNumPyに\r\n",
    "                actions = action.squeeze(1).numpy()\r\n",
    "\r\n",
    "                # 1stepの実行\r\n",
    "                for i in range(NUM_PROCESSES):\r\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(\r\n",
    "                        actions[i])\r\n",
    "\r\n",
    "                    # episodeの終了評価と、state_nextを設定\r\n",
    "                    if done_np[i]:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\r\n",
    "\r\n",
    "                        # 環境0のときのみ出力\r\n",
    "                        if i == 0:\r\n",
    "                            print('%d Episode: Finished after %d steps' % (\r\n",
    "                                episode, each_step[i]+1))\r\n",
    "                            episode += 1\r\n",
    "\r\n",
    "                        # 報酬の設定\r\n",
    "                        if each_step[i] < 195:\r\n",
    "                            reward_np[i] = -1.0  # 途中でこけたら罰則として報酬-1を与える\r\n",
    "                        else:\r\n",
    "                            reward_np[i] = 1.0  # 立ったまま終了時は報酬1を与える\r\n",
    "\r\n",
    "                        each_step[i] = 0  # step数のリセット\r\n",
    "                        obs_np[i] = envs[i].reset()  # 実行環境のリセット\r\n",
    "\r\n",
    "                    else:\r\n",
    "                        reward_np[i] = 0.0  # 普段は報酬0\r\n",
    "                        each_step[i] += 1\r\n",
    "\r\n",
    "                # 報酬をtensorに変換し、試行の総報酬に足す\r\n",
    "                reward = torch.from_numpy(reward_np).float()\r\n",
    "                episode_rewards += reward\r\n",
    "\r\n",
    "                # 各実行環境それぞれについて、doneならmaskは0に、継続中ならmaskは1にする\r\n",
    "                masks = torch.FloatTensor(\r\n",
    "                    [[0.0] if done_ else [1.0] for done_ in done_np])\r\n",
    "\r\n",
    "                # 最後の試行の総報酬を更新する\r\n",
    "                final_rewards *= masks  # 継続中の場合は1をかけ算してそのまま、done時には0を掛けてリセット\r\n",
    "                # 継続中は0を足す、done時にはepisode_rewardsを足す\r\n",
    "                final_rewards += (1 - masks) * episode_rewards\r\n",
    "\r\n",
    "                # 試行の総報酬を更新する\r\n",
    "                episode_rewards *= masks  # 継続中のmaskは1なのでそのまま、doneの場合は0に\r\n",
    "\r\n",
    "                # 現在の状態をdone時には全部0にする\r\n",
    "                current_obs *= masks\r\n",
    "\r\n",
    "                # current_obsを更新\r\n",
    "                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\r\n",
    "                current_obs = obs  # 最新のobsを格納\r\n",
    "\r\n",
    "                # メモリオブジェクトに今stepのtransitionを挿入\r\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\r\n",
    "\r\n",
    "            # advancedのfor loop終了\r\n",
    "\r\n",
    "            # advancedした最終stepの状態から予想する状態価値を計算\r\n",
    "\r\n",
    "            with torch.no_grad():\r\n",
    "                next_value = actor_critic.get_value(\r\n",
    "                    rollouts.observations[-1]).detach()\r\n",
    "                # rollouts.observationsのサイズはtorch.Size([6, 16, 4])\r\n",
    "\r\n",
    "            # 全stepの割引報酬和を計算して、rolloutsの変数returnsを更新\r\n",
    "            rollouts.compute_returns(next_value)\r\n",
    "\r\n",
    "            # ネットワークとrolloutの更新\r\n",
    "            global_brain.update(rollouts)\r\n",
    "            rollouts.after_update()\r\n",
    "\r\n",
    "            # 全部のNUM_PROCESSESが200step経ち続けたら成功\r\n",
    "            if final_rewards.sum().numpy() >= NUM_PROCESSES:\r\n",
    "                print('連続成功')\r\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# main学習\r\n",
    "cartpole_env = Environment()\r\n",
    "cartpole_env.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 Episode: Finished after 13 steps\n",
      "1 Episode: Finished after 29 steps\n",
      "2 Episode: Finished after 33 steps\n",
      "3 Episode: Finished after 64 steps\n",
      "4 Episode: Finished after 117 steps\n",
      "5 Episode: Finished after 68 steps\n",
      "6 Episode: Finished after 200 steps\n",
      "7 Episode: Finished after 170 steps\n",
      "8 Episode: Finished after 200 steps\n",
      "9 Episode: Finished after 116 steps\n",
      "10 Episode: Finished after 146 steps\n",
      "11 Episode: Finished after 144 steps\n",
      "12 Episode: Finished after 197 steps\n",
      "13 Episode: Finished after 77 steps\n",
      "14 Episode: Finished after 200 steps\n",
      "15 Episode: Finished after 27 steps\n",
      "16 Episode: Finished after 85 steps\n",
      "17 Episode: Finished after 21 steps\n",
      "18 Episode: Finished after 29 steps\n",
      "19 Episode: Finished after 200 steps\n",
      "連続成功\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('rl': conda)"
  },
  "interpreter": {
   "hash": "038d8608cabbe08bfa95b44150dc94e5c00fe384375a362fc4ac8c97693876bc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}